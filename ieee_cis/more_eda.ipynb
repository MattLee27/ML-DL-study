{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More EDA and feature engineering\n",
    "- EDA: [Extensive EDA and Modeling XGB Hyperopt](https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt)\n",
    "- modeling: [IEEE - GB-2 (make Amount useful again)](https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again)\n",
    "- feature engineering 중점 사항: 이걸 하나씩 바꿔 가면서 정확도 해보기!! submission 파일 따로 저장하고 **커밋시 메시지에 해당 내용 꼭 쓰기**\n",
    "    - null 데이터는 일단 그대로 두기\n",
    "    - P_emaildomain: boolean으로. mail.com or not\n",
    "    - R_emaildomain: boolean으로? apple.com or not\n",
    "    - id_23: ip_proxy or not? ip_proxy는 사기치려고 들어오는 사람들."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['df_id.pkl', 'df_pca.pkl', 'df_test.pkl', 'df_train.pkl', 'df_trans.pkl', 'sample_submission.csv', 'test_0823.pkl', 'test_identity.csv', 'test_transaction.csv', 'train_0823.pkl', 'train_identity.csv', 'train_transaction.csv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, sys, gc, warnings, random\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#standard plotly imports\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "#import cufflinks\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "#using plotly + cufflinks in offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "#preprocessing, modeling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "        end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645.97 Mb\n",
      "561.50 Mb\n"
     ]
    }
   ],
   "source": [
    "# PCA 안한걸로 읽기\n",
    "df_train = pd.read_pickle(\"../input/df_train.pkl\")\n",
    "df_test = pd.read_pickle(\"../input/df_test.pkl\")\n",
    "#must be 645.97\n",
    "print(\"{:1.2f} Mb\".format(df_train.memory_usage().sum() / 1024**2))\n",
    "#must be 561.50\n",
    "print(\"{:1.2f} Mb\".format(df_test.memory_usage().sum() / 1024**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가적인 전처리\n",
    "# df_train.P_emaildomain.fillna(\"NoInf\", inplace=True)\n",
    "# df_test.R_emaildomain.fillna(\"NoInf\", inplace=True)\n",
    "\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    # bin -> emails dict에 따라 매핑(수정)\n",
    "    df_train[c + '_bin'] = df_train[c].map(emails)\n",
    "    df_test[c + '_bin'] = df_test[c].map(emails)\n",
    "    \n",
    "    # suffix -> 도메인 중 맨 마지막(. 뒤에) 부분\n",
    "    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    # suffix 확인 -> us_email 값에 해당 안하면 그대로. 해당하면 'us'로\n",
    "    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>P_emaildomain_bin</th>\n",
       "      <th>P_emaildomain_suffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmail.com</td>\n",
       "      <td>google</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outlook.com</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gmail.com</td>\n",
       "      <td>google</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  P_emaildomain P_emaildomain_bin P_emaildomain_suffix\n",
       "0           NaN               NaN                  nan\n",
       "1     gmail.com            google                  com\n",
       "2   outlook.com         microsoft                  com\n",
       "3     yahoo.com             yahoo                  com\n",
       "4     gmail.com            google                  com"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "df_train[['P_emaildomain', 'P_emaildomain_bin', 'P_emaildomain_suffix']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object 형식 Label Encoding. object 형식은 모델에 들어갈 수 없음\n",
    "# 근데 숫자로 된 범주형 변수들은 안하나?\n",
    "for f in df_train.drop('isFraud', axis=1).columns:\n",
    "    if df_train[f].dtype == 'object' or df_test[f].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df_train[f].values) + list(df_test[f].values)) #더하면 어떻게 되는거지?\n",
    "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "        df_test[f] = lbl.transform(list(df_test[f].values))\n",
    "        # 이렇게 되면 NaN은 NaN으로 transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- extensive EDA에서 TransactionAmt를 표준화, log 처리 모두 하는 이유가 이해되지 않음\n",
    "- 로그 취할 경우 정규분포 꼴. 그래서 **log만 취해 보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화\n",
    "# df_train['Trans_min_std'] = (df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()) / df_train['TransactionAmt'].std()\n",
    "# df_test['Trans_min_std'] = (df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()) / df_test['TransactionAmt'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 영향 줄이기 위해 TransactionAmt는 log\n",
    "df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\n",
    "df_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 438)\n",
      "(506691, 437)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2017-12-02 00:00:00\n",
      "1   2017-12-02 00:00:01\n",
      "2   2017-12-02 00:01:09\n",
      "3   2017-12-02 00:01:39\n",
      "4   2017-12-02 00:01:46\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "0   2018-07-02 00:00:24\n",
      "1   2018-07-02 00:01:03\n",
      "2   2018-07-02 00:01:50\n",
      "3   2018-07-02 00:01:50\n",
      "4   2018-07-02 00:01:57\n",
      "Name: Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "## TransactionDT timedelta 적용하기(fraud 파일)\n",
    "import datetime\n",
    "\n",
    "START_DATE = \"2017-12-01\"\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "\n",
    "# df_train 처리\n",
    "df_train['Date'] = df_train['TransactionDT'].apply(\n",
    "lambda x: (startdate + datetime.timedelta(seconds = x))) #DT 값은 초??\n",
    "\n",
    "print(df_train['Date'].head())\n",
    "\n",
    "df_train['_Weekdays'] = df_train['Date'].dt.dayofweek\n",
    "df_train['_Hours'] = df_train['Date'].dt.hour\n",
    "df_train['_Days'] = df_train['Date'].dt.day\n",
    "\n",
    "# df_test 처리\n",
    "df_test['Date'] = df_test['TransactionDT'].apply(\n",
    "lambda x: (startdate + datetime.timedelta(seconds = x))) #DT 값은 초??\n",
    "\n",
    "print(df_test['Date'].head())\n",
    "\n",
    "df_test['_Weekdays'] = df_test['Date'].dt.dayofweek\n",
    "df_test['_Hours'] = df_test['Date'].dt.hour\n",
    "df_test['_Days'] = df_test['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533.90 Mb\n",
      "465.34 Mb\n",
      "Wall time: 3.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 여기까지만 하고 일단 데이터 저장\n",
    "df_train = pd.read_pickle(\"../input/train_0823.pkl\")\n",
    "df_test = pd.read_pickle(\"../input/test_0823.pkl\")\n",
    "#must be 533.90\n",
    "print(\"{:1.2f} Mb\".format(df_train.memory_usage().sum() / 1024**2))\n",
    "#must be 465.34\n",
    "print(\"{:1.2f} Mb\".format(df_test.memory_usage().sum() / 1024**2)) \n",
    "\n",
    "# df_train = reduce_mem_usage(df_train)\n",
    "# df_test = reduce_mem_usage(df_test)\n",
    "# df_train.to_pickle(\"../input/train_0823.pkl\")\n",
    "# df_test.to_pickle(\"../input/test_0823.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 442)\n",
      "(506691, 441)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "TARGET = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_cols = [\n",
    "    'TransactionID','TransactionDT', 'Date', #Date는 모델에 들어갈 수 없는 dtype\n",
    "    TARGET,\n",
    "]\n",
    "\n",
    "features_columns = list(df_train)\n",
    "for col in rm_cols:\n",
    "    if col in features_columns:\n",
    "        features_columns.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1, # -1 이 무슨 의미인지?\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':1,\n",
    "                    'n_estimators':800,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "\n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold:',fold_)\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n",
    "            \n",
    "        print(len(tr_x),len(vl_x))\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            vl_data = lgb.Dataset(P, label=P_y) \n",
    "        else:\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)  \n",
    "\n",
    "        estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "        pp_p = estimator.predict(P)\n",
    "        predictions += pp_p/NFOLDS\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "            print(feature_imp)\n",
    "        \n",
    "        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "        gc.collect()\n",
    "        \n",
    "    tt_df['prediction'] = predictions\n",
    "    \n",
    "    return tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\",\n",
    "                               index_col = 'TransactionID')\n",
    "\n",
    "df_test = df_test.merge(sample_submission, how='left', left_index=True,\n",
    "                         right_index=True, on='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "295270 295270\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.931643\tvalid_1's auc: 0.912555\n",
      "[400]\ttraining's auc: 0.957062\tvalid_1's auc: 0.927869\n",
      "[600]\ttraining's auc: 0.976893\tvalid_1's auc: 0.939547\n",
      "[800]\ttraining's auc: 0.987908\tvalid_1's auc: 0.947471\n",
      "[1000]\ttraining's auc: 0.99296\tvalid_1's auc: 0.951956\n",
      "[1200]\ttraining's auc: 0.995662\tvalid_1's auc: 0.954688\n",
      "[1400]\ttraining's auc: 0.997225\tvalid_1's auc: 0.956466\n",
      "[1600]\ttraining's auc: 0.998047\tvalid_1's auc: 0.957722\n",
      "[1800]\ttraining's auc: 0.998623\tvalid_1's auc: 0.958734\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1800]\ttraining's auc: 0.998623\tvalid_1's auc: 0.958734\n",
      "Fold: 1\n",
      "295270 295270\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.935022\tvalid_1's auc: 0.908718\n",
      "[400]\ttraining's auc: 0.960564\tvalid_1's auc: 0.924263\n",
      "[600]\ttraining's auc: 0.978034\tvalid_1's auc: 0.935223\n",
      "[800]\ttraining's auc: 0.988573\tvalid_1's auc: 0.94246\n",
      "[1000]\ttraining's auc: 0.993377\tvalid_1's auc: 0.947323\n",
      "[1200]\ttraining's auc: 0.996064\tvalid_1's auc: 0.950417\n",
      "[1400]\ttraining's auc: 0.997479\tvalid_1's auc: 0.952456\n",
      "[1600]\ttraining's auc: 0.998267\tvalid_1's auc: 0.953981\n",
      "[1800]\ttraining's auc: 0.998801\tvalid_1's auc: 0.95515\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1800]\ttraining's auc: 0.998801\tvalid_1's auc: 0.95515\n",
      "Wall time: 46min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if LOCAL_TEST:\n",
    "    test_predictions = make_predictions(df_train, df_test, features_columns, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "else:\n",
    "    lgb_params['learning_rate'] = 0.005\n",
    "    lgb_params['n_estimators'] = 1800\n",
    "    lgb_params['early_stopping_rounds'] = 100    \n",
    "    test_predictions = make_predictions(df_train, df_test, features_columns, TARGET, lgb_params, NFOLDS=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL_TEST:\n",
    "    test_predictions['isFraud'] = test_predictions['prediction']\n",
    "    test_predictions[['TransactionID','isFraud']].to_csv('gb2_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
