{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IEEE-CV options\n",
    "- [IEEE - CV options](https://www.kaggle.com/kyakovlev/ieee-cv-options)\n",
    "- 모델링은 lightGBM\n",
    "- 여러 cross validation 모델 모으고, 시계열 데이터 반영(뒷부분)\n",
    "- 이것도 결국 [data minification](https://www.kaggle.com/kyakovlev/ieee-data-minification) 참고한 커널. 일단 원 데이터로 해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input\\df_id.pkl\n",
      "../input\\df_pca.pkl\n",
      "../input\\df_test.pkl\n",
      "../input\\df_train.pkl\n",
      "../input\\df_trans.pkl\n",
      "../input\\sample_submission.csv\n",
      "../input\\test.pkl\n",
      "../input\\test_0823.pkl\n",
      "../input\\test_identity.csv\n",
      "../input\\test_transaction.csv\n",
      "../input\\train_0823.pkl\n",
      "../input\\train_identity.csv\n",
      "../input\\train_transaction.csv\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "# for MacBook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"../input\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 41\n",
    "seed_everything(SEED)\n",
    "TARGET = 'isFraud'\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These parameters we will keep untouched\n",
    "# for each lgbm model\n",
    "# the unique param that we will look at\n",
    "# is n_estimators\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':20000,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n"
     ]
    }
   ],
   "source": [
    "print('Load data')\n",
    "train_df = pd.read_pickle(\"../input/df_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape control: (501214, 435) (89326, 435)\n"
     ]
    }
   ],
   "source": [
    "# We will prepare simulation here\n",
    "# Last month will be our test\n",
    "train_df['DT_M'] = train_df['TransactionDT'].apply(\n",
    "    lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "train_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n",
    "\n",
    "test_df = train_df[\n",
    "    train_df['DT_M'] == train_df['DT_M'].max()].reset_index(drop=True)\n",
    "train_df = train_df[\n",
    "    train_df['DT_M'] < train_df['DT_M'].max()].reset_index(drop=True)\n",
    "    \n",
    "print('Shape control:', train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17\n",
       "1    17\n",
       "2    17\n",
       "3    17\n",
       "4    17\n",
       "Name: DT_M, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['DT_M'].head() # 원래 train에서 DT_M이 17(max, last month)인 것만 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12\n",
       "1    12\n",
       "2    12\n",
       "3    12\n",
       "4    12\n",
       "Name: DT_M, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['DT_M'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductCD\n",
      "card4\n",
      "card6\n",
      "P_emaildomain\n",
      "R_emaildomain\n",
      "M1\n",
      "M2\n",
      "M3\n",
      "M4\n",
      "M5\n",
      "M6\n",
      "M7\n",
      "M8\n",
      "M9\n",
      "id_12\n",
      "id_15\n",
      "id_16\n",
      "id_23\n",
      "id_27\n",
      "id_28\n",
      "id_29\n",
      "id_30\n",
      "id_31\n",
      "id_33\n",
      "id_34\n",
      "id_35\n",
      "id_36\n",
      "id_37\n",
      "id_38\n",
      "DeviceType\n",
      "DeviceInfo\n"
     ]
    }
   ],
   "source": [
    "# object data 타입 변경\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype=='O':\n",
    "        print(col)\n",
    "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col])+list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col]  = le.transform(test_df[col])\n",
    "        \n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "        test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Some Features\n",
    "rm_cols = [\n",
    "    'TransactionID','TransactionDT', # These columns are pure noise right now\n",
    "    TARGET,                          # Not target in features))\n",
    "    'DT_M'                           # Column that we used to simulate test set\n",
    "]\n",
    "\n",
    "# Remove V columns (for faster training)\n",
    "rm_cols += ['V'+str(i) for i in range(1,340)] #V1 ~ V339 제외시킴\n",
    "# print(rm_cols)\n",
    "\n",
    "# Final features\n",
    "features_columns = [col for col in list(train_df) if col not in rm_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV(cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = test_df[['TransactionID',TARGET]]\n",
    "\n",
    "# We will always use same number of splits\n",
    "# for training model\n",
    "# Number of splits depends on data structure\n",
    "# and in our case it is better to use \n",
    "# something in range 5-10\n",
    "# 5 - is a common number of splits\n",
    "# 10+ is too much (we will not have enough diversity in data)\n",
    "# Here we will use 3 for faster training\n",
    "# but you can change it by yourself\n",
    "N_SPLITS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. No validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "No Validation training... 500 boosting rounds\n",
      "AUC score 0.9277271781014739\n",
      "####################\n",
      "####################\n",
      "No Validation training... 1000 boosting rounds\n",
      "AUC score 0.9326734191953692\n",
      "####################\n",
      "####################\n",
      "No Validation training... 2500 boosting rounds\n",
      "AUC score 0.9331337450590427\n",
      "####################\n",
      "####################\n",
      "No Validation training... 5000 boosting rounds\n",
      "AUC score 0.9306524679300963\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "# Main Data\n",
    "# We will take whole train data set\n",
    "# and will NOT use any early stopping \n",
    "X,y = train_df[features_columns], train_df[TARGET]\n",
    "\n",
    "# Test Data (what we need to predict)\n",
    "P = test_df[features_columns]\n",
    "\n",
    "# We don't know where to stop\n",
    "# so we will try to guess \n",
    "# number of boosting rounds\n",
    "for n_rounds in [500,1000,2500,5000]:\n",
    "    print('#'*20)\n",
    "    print('No Validation training...', n_rounds, 'boosting rounds')\n",
    "    corrected_lgb_params = lgb_params.copy()\n",
    "    corrected_lgb_params['n_estimators'] = n_rounds\n",
    "    corrected_lgb_params['early_stopping_rounds'] = None\n",
    "\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    \n",
    "    estimator = lgb.train(\n",
    "                corrected_lgb_params,\n",
    "                train_data\n",
    "            )\n",
    "\n",
    "    RESULTS['no_validation_'+str(n_rounds)] = estimator.predict(P)\n",
    "    print('AUC score', metrics.roc_auc_score(RESULTS[TARGET], RESULTS['no_validation_'+str(n_rounds)]))\n",
    "    print('#'*20)\n",
    "    \n",
    "# Be careful. We are printing auc results\n",
    "# for our simulated test set\n",
    "# but in real Data set we do not have True labels (obviously)\n",
    "# and can't be sure that we stopped in right round\n",
    "# lb probing can give you some idea how good our training is\n",
    "# but this leads to nowhere -> overfits or completely bad results\n",
    "# bad practice for real life problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#'*20)\n",
    "print('KFold training...')\n",
    "\n",
    "# You can find oof name for this strategy\n",
    "# oof - Out Of Fold\n",
    "# as we will use one fold as validation\n",
    "# and stop training when validation metric\n",
    "# stops improve\n",
    "from sklearn.model_selection import KFold\n",
    "folds = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Main Data\n",
    "X,y = train_df[features_columns], train_df[TARGET]\n",
    "\n",
    "# Test Data\n",
    "P = test_df[features_columns]\n",
    "RESULTS['kfold'] = 0\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "    print('Fold:',fold_+1)\n",
    "    tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]    \n",
    "    vl_x, v_y = X.iloc[val_idx,:], y[val_idx]    \n",
    "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    valid_data = lgb.Dataset(vl_x, label=v_y)  \n",
    "\n",
    "    estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            train_data,\n",
    "            valid_sets = [train_data, valid_data],\n",
    "            verbose_eval = 1000,\n",
    "        )\n",
    "\n",
    "    RESULTS['kfold'] = estimator.predict(P)\n",
    "\n",
    "print('AUC score', metrics.roc_auc_score(RESULTS[TARGET], RESULTS['kfold']))\n",
    "print('#'*20)\n",
    "\n",
    "## We have two \"problems\" here\n",
    "## 1st: Training score goes upto 1 and it's not normal situation\n",
    "## It's nomally means that model did perfect or\n",
    "## almost perfect match between \"data fingerprint\" and target\n",
    "## we definitely should stop before to generalize better\n",
    "## 2nd: Our LB probing gave 0.936 and it is too far away from validation score\n",
    "## some difference is normal, but such gap is too big"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
