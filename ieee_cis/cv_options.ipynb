{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IEEE-CV options\n",
    "- [IEEE - CV options](https://www.kaggle.com/kyakovlev/ieee-cv-options)\n",
    "- 모델링은 lightGBM\n",
    "- 여러 cross validation 모델 모으고, 시계열 데이터 반영(뒷부분)\n",
    "- 이것도 결국 [data minification](https://www.kaggle.com/kyakovlev/ieee-data-minification) 참고한 커널. 일단 원 데이터로 해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input\\df_id.pkl\n",
      "../input\\df_pca.pkl\n",
      "../input\\df_test.pkl\n",
      "../input\\df_train.pkl\n",
      "../input\\df_trans.pkl\n",
      "../input\\sample_submission.csv\n",
      "../input\\test.pkl\n",
      "../input\\test_0823.pkl\n",
      "../input\\test_identity.csv\n",
      "../input\\test_transaction.csv\n",
      "../input\\train_0823.pkl\n",
      "../input\\train_identity.csv\n",
      "../input\\train_transaction.csv\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"../input\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 41\n",
    "seed_everything(SEED)\n",
    "TARGET = 'isFraud'\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These parameters we will keep untouched\n",
    "# for each lgbm model\n",
    "# the unique param that we will look at\n",
    "# is n_estimators\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':20000,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n"
     ]
    }
   ],
   "source": [
    "print('Load data')\n",
    "train_df = pd.read_pickle(\"../input/df_train.pkl\")\n",
    "test_df = pd.read_pickle(\"../input/df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape control: (501214, 435) (89326, 435)\n"
     ]
    }
   ],
   "source": [
    "# We will prepare simulation here\n",
    "# Last month will be our test test \n",
    "# train_df['DT_M'] = train_df['TransactionDT'].apply(\n",
    "#     lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "# train_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n",
    "\n",
    "# test_df = train_df[\n",
    "#     train_df['DT_M'] == train_df['DT_M'].max()].reset_index(drop=True)\n",
    "# train_df = train_df[\n",
    "#     train_df['DT_M'] < train_df['DT_M'].max()].reset_index(drop=True)\n",
    "    \n",
    "print('Shape control:', train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17\n",
       "1    17\n",
       "2    17\n",
       "3    17\n",
       "4    17\n",
       "Name: DT_M, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['DT_M'].head() # 원래 train에서 DT_M이 17(max)인 것만 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12\n",
       "1    12\n",
       "2    12\n",
       "3    12\n",
       "4    12\n",
       "Name: DT_M, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['DT_M'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductCD\n",
      "card4\n",
      "card6\n",
      "P_emaildomain\n",
      "R_emaildomain\n",
      "M1\n",
      "M2\n",
      "M3\n",
      "M4\n",
      "M5\n",
      "M6\n",
      "M7\n",
      "M8\n",
      "M9\n",
      "id_12\n",
      "id_15\n",
      "id_16\n",
      "id_23\n",
      "id_27\n",
      "id_28\n",
      "id_29\n",
      "id_30\n",
      "id_31\n",
      "id_33\n",
      "id_34\n",
      "id_35\n",
      "id_36\n",
      "id_37\n",
      "id_38\n",
      "DeviceType\n",
      "DeviceInfo\n"
     ]
    }
   ],
   "source": [
    "# object data 타입 변경\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype=='O':\n",
    "        print(col)\n",
    "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col])+list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col]  = le.transform(test_df[col])\n",
    "        \n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "        test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Some Features\n",
    "rm_cols = [\n",
    "    'TransactionID','TransactionDT', # These columns are pure noise right now\n",
    "    TARGET,                          # Not target in features))\n",
    "    'DT_M'                           # Column that we used to simulate test set\n",
    "]\n",
    "\n",
    "# Remove V columns (for faster training)\n",
    "rm_cols += ['V'+str(i) for i in range(1,340)] #V1 ~ V339 제외시킴\n",
    "# print(rm_cols)\n",
    "\n",
    "# Final features\n",
    "features_columns = [col for col in list(train_df) if col not in rm_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV(cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = test_df[['TransactionID',TARGET]]\n",
    "\n",
    "# We will always use same number of splits\n",
    "# for training model\n",
    "# Number of splits depends on data structure\n",
    "# and in our case it is better to use \n",
    "# something in range 5-10\n",
    "# 5 - is a common number of splits\n",
    "# 10+ is too much (we will not have enough diversity in data)\n",
    "# Here we will use 3 for faster training\n",
    "# but you can change it by yourself\n",
    "N_SPLITS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. No validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
