{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM 적용한 파일\n",
    "- fraud_modeling에서 전처리한 파일 이용\n",
    "- 몇개 칼럼 중 소수 값은 Other로 대체하고, V1~V339는 30개 column으로 PCA 한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, sys, gc, warnings, random\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#standard plotly imports\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "#import cufflinks\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "#using plotly + cufflinks in offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "#preprocessing, modeling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "        end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df.to_pickle(\"../input/df_pca.pkl\")\n",
    "df = pd.read_pickle(\"../input/df_pca.pkl\")\n",
    "# df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting train and test back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 134)\n",
      "(506691, 133)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop(\n",
    "'isFraud', axis=1)\n",
    "\n",
    "print(df_train.shape) # 정규화 과정에서 한 과정 합쳤기 때문에 열이 하나 적음\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM modeling with 4 threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\",\n",
    "                               index_col = 'TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 421\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "TARGET = 'isFraud'\n",
    "\n",
    "rm_cols = [\n",
    "    'TransactionID', 'TransactionDT', TARGET\n",
    "]\n",
    "\n",
    "# 참고: list(X_train) 하면 칼럼명들이 나옴\n",
    "# rm_cols에 해당하는 칼럼 제거\n",
    "features_columns = list(df_train)\n",
    "for col in rm_cols:\n",
    "    if col in features_columns:\n",
    "        features_columns.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'boosting_type':'gbdt',\n",
    "    'metric':'auc',\n",
    "    'n_jobs':4, #cpu 개수\n",
    "    'learning_rate':0.01,\n",
    "    'num_leaves': 2**8,\n",
    "    'max_depth':-1,\n",
    "    'tree_learner':'serial',\n",
    "    'colsample_bytree': 0.7,\n",
    "    'subsample_freq':1,\n",
    "    'subsample':1,\n",
    "    'n_estimators':800,\n",
    "    'max_bin':255,\n",
    "    'verbose':-1,\n",
    "    'seed': SEED,\n",
    "    'early_stopping_rounds':100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting, prediction 같이 하는 함수 -> train, test set 모두 필요\n",
    "def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "\n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold:',fold_)\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n",
    "            \n",
    "        print(len(tr_x),len(vl_x))\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            vl_data = lgb.Dataset(P, label=P_y) \n",
    "        else:\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)  \n",
    "\n",
    "        estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200\n",
    "        )   \n",
    "        \n",
    "        pp_p = estimator.predict(P)\n",
    "        predictions += pp_p/NFOLDS\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "            print(feature_imp)\n",
    "        \n",
    "        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "        gc.collect()\n",
    "        \n",
    "    tt_df['prediction']  = predictions\n",
    "    \n",
    "    return tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.merge(sample_submission, how='left', left_index=True,\n",
    "                         right_index=True, on='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "492116 98424\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.954775\tvalid_1's auc: 0.932252\n",
      "[400]\ttraining's auc: 0.980755\tvalid_1's auc: 0.950727\n",
      "[600]\ttraining's auc: 0.990626\tvalid_1's auc: 0.95852\n",
      "[800]\ttraining's auc: 0.994687\tvalid_1's auc: 0.961858\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's auc: 0.994687\tvalid_1's auc: 0.961858\n",
      "Fold: 1\n",
      "492116 98424\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.954439\tvalid_1's auc: 0.931079\n",
      "[400]\ttraining's auc: 0.980927\tvalid_1's auc: 0.949358\n",
      "[600]\ttraining's auc: 0.990623\tvalid_1's auc: 0.956299\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if LOCAL_TEST:\n",
    "    test_predictions = make_predictions(df_train, df_test,\n",
    "                        features_columns, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "else:\n",
    "    lgb_params['learning_rate'] = 0.01\n",
    "    lgb_params['n_estimators'] = 800\n",
    "    lgb_params['early_stopping_rounds'] = 100    \n",
    "    test_predictions = make_predictions(df_train, df_test, features_columns, TARGET, lgb_params, NFOLDS=6) #NFOLDS=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOCAL_TEST 는 어디서 True로 변하는 건지?\n",
    "\n",
    "########################### Export\n",
    "if not LOCAL_TEST:\n",
    "    test_predictions['isFraud'] = test_predictions['prediction']\n",
    "    test_predictions[['TransactionID','isFraud']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
