{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling on Gram\n",
    "- 지금까지 공개된 FE 노하우를 총정리한 커널: [CIS Fraud Detection(Visualize+Feature Engineering)](https://www.kaggle.com/ysjf13/cis-fraud-detection-visualize-feature-engineering)\n",
    "- 결과: 0.9476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/df_id.pkl\n",
      "../input/df_pca.pkl\n",
      "../input/df_test.pkl\n",
      "../input/df_train.pkl\n",
      "../input/df_trans.pkl\n",
      "../input/sample_submission.csv\n",
      "../input/second_test.pkl\n",
      "../input/second_train.pkl\n",
      "../input/test.pkl\n",
      "../input/test_0823.pkl\n",
      "../input/test_identity.csv\n",
      "../input/test_transaction.csv\n",
      "../input/train_0823.pkl\n",
      "../input/train_identity.csv\n",
      "../input/train_transaction.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc, os, pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"../input/\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\tSuccessfully loaded train_identity!\n",
      "\tSuccessfully loaded train_transaction!\n",
      "\tSuccessfully loaded test_identity!\n",
      "\tSuccessfully loaded test_transaction!\n",
      "\tSuccessfully loaded sample_submission!\n",
      "Data was successfully loaded!\n",
      "\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "folder_path = '../input/'\n",
    "print('Loading data...')\n",
    "\n",
    "train_identity = pd.read_csv(f'{folder_path}train_identity.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded train_identity!')\n",
    "\n",
    "train_transaction = pd.read_csv(f'{folder_path}train_transaction.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded train_transaction!')\n",
    "\n",
    "test_identity = pd.read_csv(f'{folder_path}test_identity.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded test_identity!')\n",
    "\n",
    "test_transaction = pd.read_csv(f'{folder_path}test_transaction.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded test_transaction!')\n",
    "\n",
    "sub = pd.read_csv(f'{folder_path}sample_submission.csv')\n",
    "print('\\tSuccessfully loaded sample_submission!')\n",
    "\n",
    "print('Data was successfully loaded!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n",
      "Data was successfully merged!\n",
      "\n",
      "Train dataset has 590540 rows and 433 columns.\n",
      "Test dataset has 506691 rows and 432 columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Merging data...')\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "print('Data was successfully merged!\\n')\n",
    "\n",
    "del train_identity, train_transaction, test_identity, test_transaction\n",
    "\n",
    "print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n",
    "print(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1975.37 MB\n",
      "Memory usage after optimization is: 547.82 MB\n",
      "Decreased by 72.3%\n",
      "Memory usage of dataframe is 1693.87 MB\n",
      "Memory usage after optimization is: 480.15 MB\n",
      "Decreased by 71.7%\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.to_pickle(\"../input/third_train.pkl\")\n",
    "test.to_pickle(\"../input/third_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "\n",
    "for df in [train, test]:\n",
    "    # Temporary\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n",
    "    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n",
    "    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n",
    "    \n",
    "    df['DT_hour'] = df['DT'].dt.hour\n",
    "    df['DT_day_week'] = df['DT'].dt.dayofweek\n",
    "    df['DT_day'] = df['DT'].dt.day\n",
    "    \n",
    "    # D9 column. 이건 왜 하는지 모르겠지만 일단 해 놓기\n",
    "    df['D9'] = np.where(df['D9'].isna(),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 440)\n",
      "(506691, 439)\n"
     ]
    }
   ],
   "source": [
    "# 칼럼 수 확인 -> 440개\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card 컬럼들 이용한 uid 생성\n",
    "def addNewFeatures(data): \n",
    "    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n",
    "    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n",
    "    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = addNewFeatures(train)\n",
    "test = addNewFeatures(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n",
    "\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean','std']:\n",
    "        # i_cols 그룹별 평균값을 가지는 칼럼 생성. 딱히 칼럼 제거하진 않는 듯\n",
    "        new_col_name = col + '_TransactionAmt_' + agg_type\n",
    "        temp_df = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n",
    "        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "                                                columns={agg_type: new_col_name})\n",
    "\n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "        train[new_col_name] = train[col].map(temp_df)\n",
    "        test[new_col_name]  = test[col].map(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df.replace -> 여기서 이 코드는 무슨 의미인지??\n",
    "train = train.replace(np.inf,999)\n",
    "test = test.replace(np.inf,999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리기반 모델 -> 여기서 log 처리한다고 달라질 것 같진 않음. 일단 해보기\n",
    "train['TransactionAmt'] = np.log1p(train['TransactionAmt'])\n",
    "test['TransactionAmt'] = np.log1p(test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 FE 1. email mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n",
    "          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n",
    "          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n",
    "          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n",
    "          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n",
    "          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n",
    "          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n",
    "          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n",
    "          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n",
    "          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n",
    "          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train[c + '_bin'] = train[c].map(emails)\n",
    "    test[c + '_bin'] = test[c].map(emails)\n",
    "    \n",
    "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 FE 2. Browser mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"lastest_browser\"] = np.zeros(train.shape[0])\n",
    "test[\"lastest_browser\"] = np.zeros(test.shape[0])\n",
    "\n",
    "def setBrowser(df):\n",
    "    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n",
    "    return df\n",
    "\n",
    "train = setBrowser(train)\n",
    "test = setBrowser(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 FE 3. Device mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setDevice(df):\n",
    "    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "    \n",
    "    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "\n",
    "    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "\n",
    "    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n",
    "    df['had_id'] = 1\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = setDevice(train)\n",
    "test = setDevice(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Encoding?\n",
    "i_cols = ['card1','card2','card3','card5',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'DeviceInfo','device_name',\n",
    "          'id_30','id_33',\n",
    "          'uid','uid2','uid3',\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([train[[col]], test[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n",
    "    train[col+'_fq_enc'] = train[col].map(fq_encode)\n",
    "    test[col+'_fq_enc']  = test[col].map(fq_encode)\n",
    "\n",
    "\n",
    "for col in ['DT_M','DT_W','DT_D']:\n",
    "    temp_df = pd.concat([train[[col]], test[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "            \n",
    "    train[col+'_total'] = train[col].map(fq_encode)\n",
    "    test[col+'_total']  = test[col].map(fq_encode)\n",
    "\n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "i_cols = ['uid']\n",
    "\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        new_column = col + '_' + period\n",
    "            \n",
    "        temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n",
    "        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n",
    "        fq_encode = temp_df[new_column].value_counts().to_dict()\n",
    "            \n",
    "        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n",
    "        test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n",
    "        \n",
    "        train[new_column] /= train[period+'_total']\n",
    "        test[new_column]  /= test[period+'_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionDT', 'TransactionAmt', 'card1_TransactionAmt_mean',\n",
       "       'card1_TransactionAmt_std', 'card2_TransactionAmt_mean',\n",
       "       'card2_TransactionAmt_std', 'card3_TransactionAmt_mean',\n",
       "       'card3_TransactionAmt_std', 'card5_TransactionAmt_mean',\n",
       "       'card5_TransactionAmt_std', 'uid_TransactionAmt_mean',\n",
       "       'uid_TransactionAmt_std', 'uid2_TransactionAmt_mean',\n",
       "       'uid2_TransactionAmt_std', 'uid3_TransactionAmt_mean',\n",
       "       'uid3_TransactionAmt_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns[train.columns.str.contains('Transaction')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy columns 제거? 이 부분을 일단 안함!!\n",
    "noisy_cols = [#'TransactionID',\n",
    "    'TransactionDT', # Not target in features\n",
    "    'uid','uid2','uid3',                                 \n",
    "    'DT','DT_M','DT_W','DT_D',       \n",
    "    'DT_hour','DT_day_week','DT_day',\n",
    "    'DT_D_total','DT_W_total','DT_M_total',\n",
    "    'id_30','id_31','id_33',\n",
    "    'D1', 'D2', 'D9',\n",
    "]\n",
    "\n",
    "noisy_cat_cols = list(train[noisy_cols].select_dtypes(include=['object']).columns) \n",
    "noisy_num_cold = list(train[noisy_cols].select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting uid\n",
      "Deleting uid2\n",
      "Deleting uid3\n",
      "Deleting id_30\n",
      "Deleting id_31\n",
      "Deleting id_33\n",
      "Deleting TransactionDT\n",
      "Deleting DT\n",
      "Deleting DT_M\n",
      "Deleting DT_W\n",
      "Deleting DT_D\n",
      "Deleting DT_hour\n",
      "Deleting DT_day_week\n",
      "Deleting DT_day\n",
      "Deleting DT_D_total\n",
      "Deleting DT_W_total\n",
      "Deleting DT_M_total\n",
      "Deleting D1\n",
      "Deleting D2\n",
      "Deleting D9\n"
     ]
    }
   ],
   "source": [
    "cat_attr = list(train.select_dtypes(include=['object']).columns)\n",
    "num_attr = list(train.select_dtypes(exclude=['object']).columns)\n",
    "num_attr.remove('isFraud')\n",
    "\n",
    "for col in noisy_cat_cols:\n",
    "    if col in cat_attr:\n",
    "        print(\"Deleting \" + col)\n",
    "        cat_attr.remove(col)\n",
    "for col in noisy_num_cold:\n",
    "    if col in num_attr:\n",
    "        print(\"Deleting \" + col)\n",
    "        num_attr.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# object data -> Label Encoding(모델링 위해)\n",
    "# 그냥 팁: pandas에서 object보다 category 형 변수가 계산 빠름(변성윤님 블로그)?\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n",
    "        train[col] = le.transform(list(train[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.006883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 여기서 TransactionDT 빼는 이유는 뭐지?? -> dtype 안 맞기 때문??\n",
    "# 일단 DT는 맞는 dtype이 아니라서 제거\n",
    "# 특이하게 여기서 시간이 몇 분 걸렸음.\n",
    "# 왜 실행이 안 되어 있지? 그램에서 pull 하지 말기.\n",
    "X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'DT'], axis=1)\n",
    "y = train.sort_values('TransactionDT')['isFraud']\n",
    "\n",
    "X_test = test.drop(['TransactionDT', 'DT'], axis=1)\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 506)\n",
      "(506691, 506)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.961565\tvalid_1's auc: 0.892356\n",
      "[400]\ttraining's auc: 0.981396\tvalid_1's auc: 0.905383\n",
      "[600]\ttraining's auc: 0.991677\tvalid_1's auc: 0.914151\n",
      "[800]\ttraining's auc: 0.99651\tvalid_1's auc: 0.91954\n",
      "[1000]\ttraining's auc: 0.998625\tvalid_1's auc: 0.923157\n",
      "[1200]\ttraining's auc: 0.999484\tvalid_1's auc: 0.924787\n",
      "[1400]\ttraining's auc: 0.99981\tvalid_1's auc: 0.926144\n",
      "[1600]\ttraining's auc: 0.999931\tvalid_1's auc: 0.926776\n",
      "[1800]\ttraining's auc: 0.999978\tvalid_1's auc: 0.927018\n",
      "[2000]\ttraining's auc: 0.999994\tvalid_1's auc: 0.927309\n",
      "[2200]\ttraining's auc: 0.999998\tvalid_1's auc: 0.927509\n",
      "[2400]\ttraining's auc: 1\tvalid_1's auc: 0.927608\n",
      "[2600]\ttraining's auc: 1\tvalid_1's auc: 0.927528\n",
      "[2800]\ttraining's auc: 1\tvalid_1's auc: 0.927523\n",
      "Early stopping, best iteration is:\n",
      "[2407]\ttraining's auc: 1\tvalid_1's auc: 0.927663\n",
      "Fold 1 | AUC: 0.9276628290296187\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.960793\tvalid_1's auc: 0.916247\n",
      "[400]\ttraining's auc: 0.980703\tvalid_1's auc: 0.928918\n",
      "[600]\ttraining's auc: 0.991983\tvalid_1's auc: 0.936794\n",
      "[800]\ttraining's auc: 0.996918\tvalid_1's auc: 0.940241\n",
      "[1000]\ttraining's auc: 0.998895\tvalid_1's auc: 0.942433\n",
      "[1200]\ttraining's auc: 0.999627\tvalid_1's auc: 0.943508\n",
      "[1400]\ttraining's auc: 0.999876\tvalid_1's auc: 0.944227\n",
      "[1600]\ttraining's auc: 0.99996\tvalid_1's auc: 0.944784\n",
      "[1800]\ttraining's auc: 0.999989\tvalid_1's auc: 0.94487\n",
      "[2000]\ttraining's auc: 0.999997\tvalid_1's auc: 0.944882\n",
      "[2200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.944854\n",
      "Early stopping, best iteration is:\n",
      "[1826]\ttraining's auc: 0.99999\tvalid_1's auc: 0.944952\n",
      "Fold 2 | AUC: 0.9449522973579755\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.962697\tvalid_1's auc: 0.916637\n",
      "[400]\ttraining's auc: 0.981796\tvalid_1's auc: 0.927847\n",
      "[600]\ttraining's auc: 0.992336\tvalid_1's auc: 0.934611\n",
      "[800]\ttraining's auc: 0.996944\tvalid_1's auc: 0.937496\n",
      "[1000]\ttraining's auc: 0.998888\tvalid_1's auc: 0.938568\n",
      "[1200]\ttraining's auc: 0.99961\tvalid_1's auc: 0.939219\n",
      "[1400]\ttraining's auc: 0.999867\tvalid_1's auc: 0.939544\n",
      "[1600]\ttraining's auc: 0.999957\tvalid_1's auc: 0.939573\n",
      "[1800]\ttraining's auc: 0.999987\tvalid_1's auc: 0.939634\n",
      "[2000]\ttraining's auc: 0.999997\tvalid_1's auc: 0.939707\n",
      "[2200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.939656\n",
      "[2400]\ttraining's auc: 1\tvalid_1's auc: 0.939634\n",
      "Early stopping, best iteration is:\n",
      "[1982]\ttraining's auc: 0.999996\tvalid_1's auc: 0.939732\n",
      "Fold 3 | AUC: 0.9397318738441324\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.959753\tvalid_1's auc: 0.92917\n",
      "[400]\ttraining's auc: 0.980904\tvalid_1's auc: 0.942653\n",
      "[600]\ttraining's auc: 0.992164\tvalid_1's auc: 0.949876\n",
      "[800]\ttraining's auc: 0.996954\tvalid_1's auc: 0.952739\n",
      "[1000]\ttraining's auc: 0.998906\tvalid_1's auc: 0.95407\n",
      "[1200]\ttraining's auc: 0.999627\tvalid_1's auc: 0.954819\n",
      "[1400]\ttraining's auc: 0.999879\tvalid_1's auc: 0.955289\n",
      "[1600]\ttraining's auc: 0.99996\tvalid_1's auc: 0.955585\n",
      "[1800]\ttraining's auc: 0.999989\tvalid_1's auc: 0.955748\n",
      "[2000]\ttraining's auc: 0.999997\tvalid_1's auc: 0.955824\n",
      "[2200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.955791\n",
      "[2400]\ttraining's auc: 1\tvalid_1's auc: 0.955666\n",
      "Early stopping, best iteration is:\n",
      "[2056]\ttraining's auc: 0.999998\tvalid_1's auc: 0.955842\n",
      "Fold 4 | AUC: 0.9558415704954626\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.961956\tvalid_1's auc: 0.905422\n",
      "[400]\ttraining's auc: 0.98124\tvalid_1's auc: 0.920628\n",
      "[600]\ttraining's auc: 0.991991\tvalid_1's auc: 0.927947\n",
      "[800]\ttraining's auc: 0.996836\tvalid_1's auc: 0.930913\n",
      "[1000]\ttraining's auc: 0.998818\tvalid_1's auc: 0.932244\n",
      "[1200]\ttraining's auc: 0.999576\tvalid_1's auc: 0.932928\n",
      "[1400]\ttraining's auc: 0.999853\tvalid_1's auc: 0.933274\n",
      "[1600]\ttraining's auc: 0.999954\tvalid_1's auc: 0.93375\n",
      "[1800]\ttraining's auc: 0.999986\tvalid_1's auc: 0.934166\n",
      "[2000]\ttraining's auc: 0.999996\tvalid_1's auc: 0.934454\n",
      "[2200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.934688\n",
      "[2400]\ttraining's auc: 1\tvalid_1's auc: 0.93491\n",
      "[2600]\ttraining's auc: 1\tvalid_1's auc: 0.935046\n",
      "[2800]\ttraining's auc: 1\tvalid_1's auc: 0.93522\n",
      "[3000]\ttraining's auc: 1\tvalid_1's auc: 0.935349\n",
      "[3200]\ttraining's auc: 1\tvalid_1's auc: 0.935409\n",
      "[3400]\ttraining's auc: 1\tvalid_1's auc: 0.935358\n",
      "Early stopping, best iteration is:\n",
      "[3098]\ttraining's auc: 1\tvalid_1's auc: 0.935439\n",
      "Fold 5 | AUC: 0.9354386021267659\n",
      "\n",
      "Mean AUC = 0.9407254345707909\n",
      "Out of folds AUC = 0.9403052675655083\n",
      "Wall time: 3h 35min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NFOLDS = 5\n",
    "folds = KFold(n_splits=NFOLDS)\n",
    "\n",
    "columns = X.columns\n",
    "splits = folds.split(X, y)\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X.shape[0])\n",
    "score = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "  \n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n",
    "    \n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n",
    "    \n",
    "    y_pred_valid = clf.predict(X_valid)\n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n",
    "    \n",
    "    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n",
    "    y_preds += clf.predict(X_test) / NFOLDS\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"\\nMean AUC = {score}\")\n",
    "print(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 근데 위에서 제거하려고 했던(list) 칼럼들 모두 제거하진 않았음(drop)\n",
    "sub['isFraud'] = y_preds\n",
    "sub.to_csv(\"0916_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance 확인\n",
    "feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n",
    "feature_importances.to_csv('feature_importances_new.csv')\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n",
    "plt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
