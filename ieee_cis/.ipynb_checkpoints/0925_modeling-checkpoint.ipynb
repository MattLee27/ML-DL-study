{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.95 넘은 커널?\n",
    "- [ieee models](https://www.kaggle.com/whitebird/a-method-to-valid-offline-lb-9506)\n",
    "- 일단 커널 안보고 0916에서 피쳐 200개 정도만 걸러서 해보기(top 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/df_id.pkl\n",
      "../input/df_pca.pkl\n",
      "../input/df_test.pkl\n",
      "../input/df_train.pkl\n",
      "../input/df_trans.pkl\n",
      "../input/sample_submission.csv\n",
      "../input/second_test.pkl\n",
      "../input/second_train.pkl\n",
      "../input/test.pkl\n",
      "../input/test_0823.pkl\n",
      "../input/test_identity.csv\n",
      "../input/test_transaction.csv\n",
      "../input/third_test.pkl\n",
      "../input/third_train.pkl\n",
      "../input/train_0823.pkl\n",
      "../input/train_identity.csv\n",
      "../input/train_transaction.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc, os, pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"../input/\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_pickle(\"../input/third_train.pkl\")\n",
    "test = pd.read_pickle(\"../input/third_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>feature</th>\n",
       "      <th>fold_1</th>\n",
       "      <th>fold_2</th>\n",
       "      <th>fold_3</th>\n",
       "      <th>fold_4</th>\n",
       "      <th>fold_5</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TransactionAmt</td>\n",
       "      <td>29862</td>\n",
       "      <td>21293</td>\n",
       "      <td>23674</td>\n",
       "      <td>24304</td>\n",
       "      <td>39402</td>\n",
       "      <td>27707.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>433</td>\n",
       "      <td>DT_D</td>\n",
       "      <td>30241</td>\n",
       "      <td>20986</td>\n",
       "      <td>23890</td>\n",
       "      <td>24349</td>\n",
       "      <td>36839</td>\n",
       "      <td>27261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>uid3_fq_enc</td>\n",
       "      <td>27202</td>\n",
       "      <td>20046</td>\n",
       "      <td>22095</td>\n",
       "      <td>22684</td>\n",
       "      <td>34150</td>\n",
       "      <td>25235.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>452</td>\n",
       "      <td>uid3_TransactionAmt_mean</td>\n",
       "      <td>25555</td>\n",
       "      <td>19780</td>\n",
       "      <td>20912</td>\n",
       "      <td>22175</td>\n",
       "      <td>34923</td>\n",
       "      <td>24669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>502</td>\n",
       "      <td>DT_D_total</td>\n",
       "      <td>24200</td>\n",
       "      <td>17897</td>\n",
       "      <td>19359</td>\n",
       "      <td>20213</td>\n",
       "      <td>36079</td>\n",
       "      <td>23549.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                   feature  fold_1  fold_2  fold_3  fold_4  \\\n",
       "0             0            TransactionAmt   29862   21293   23674   24304   \n",
       "433         433                      DT_D   30241   20986   23890   24349   \n",
       "499         499               uid3_fq_enc   27202   20046   22095   22684   \n",
       "452         452  uid3_TransactionAmt_mean   25555   19780   20912   22175   \n",
       "502         502                DT_D_total   24200   17897   19359   20213   \n",
       "\n",
       "     fold_5  average  \n",
       "0     39402  27707.0  \n",
       "433   36839  27261.0  \n",
       "499   34150  25235.4  \n",
       "452   34923  24669.0  \n",
       "502   36079  23549.6  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.read_csv(\"feature_importances_new.csv\")\n",
    "feature_importances.sort_values(by = 'average', ascending = False,\n",
    "                               inplace = True)\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "\n",
    "for df in [train, test]:\n",
    "    # Temporary\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n",
    "    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n",
    "    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n",
    "    \n",
    "    df['DT_hour'] = df['DT'].dt.hour\n",
    "    df['DT_day_week'] = df['DT'].dt.dayofweek\n",
    "    df['DT_day'] = df['DT'].dt.day\n",
    "    \n",
    "    # D9 column. 이건 왜 하는지 모르겠지만 일단 해 놓기\n",
    "    df['D9'] = np.where(df['D9'].isna(),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 440)\n",
      "(506691, 439)\n"
     ]
    }
   ],
   "source": [
    "# 칼럼 수 확인 -> 440개\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card 컬럼들 이용한 uid 생성\n",
    "def addNewFeatures(data): \n",
    "    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n",
    "    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n",
    "    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n",
    "    return data\n",
    "\n",
    "train = addNewFeatures(train)\n",
    "test = addNewFeatures(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n",
    "\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean','std']:\n",
    "        # i_cols 그룹별 평균값을 가지는 칼럼 생성. 딱히 칼럼 제거하진 않는 듯\n",
    "        new_col_name = col + '_TransactionAmt_' + agg_type\n",
    "        temp_df = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n",
    "        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "                                                columns={agg_type: new_col_name})\n",
    "\n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "        train[new_col_name] = train[col].map(temp_df)\n",
    "        test[new_col_name]  = test[col].map(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df.replace -> inf 값이 있나? 이걸 999로 대체\n",
    "train = train.replace(np.inf,999)\n",
    "test = test.replace(np.inf,999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FE 1. email mapping\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n",
    "          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n",
    "          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n",
    "          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n",
    "          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n",
    "          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n",
    "          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n",
    "          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n",
    "          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n",
    "          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n",
    "          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train[c + '_bin'] = train[c].map(emails)\n",
    "    test[c + '_bin'] = test[c].map(emails)\n",
    "    \n",
    "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FE 2. Browser engineering\n",
    "train[\"lastest_browser\"] = np.zeros(train.shape[0])\n",
    "test[\"lastest_browser\"] = np.zeros(test.shape[0])\n",
    "\n",
    "def setBrowser(df):\n",
    "    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n",
    "    return df\n",
    "\n",
    "train = setBrowser(train)\n",
    "test = setBrowser(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FE 3. Device Mapping\n",
    "def setDevice(df):\n",
    "    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "    \n",
    "    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "\n",
    "    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "\n",
    "    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n",
    "    df['had_id'] = 1\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = setDevice(train)\n",
    "test = setDevice(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Encoding?\n",
    "i_cols = ['card1','card2','card3','card5',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'DeviceInfo','device_name',\n",
    "          'id_30','id_33',\n",
    "          'uid','uid2','uid3',\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([train[[col]], test[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n",
    "    train[col+'_fq_enc'] = train[col].map(fq_encode)\n",
    "    test[col+'_fq_enc']  = test[col].map(fq_encode)\n",
    "\n",
    "\n",
    "for col in ['DT_M','DT_W','DT_D']:\n",
    "    temp_df = pd.concat([train[[col]], test[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "            \n",
    "    train[col+'_total'] = train[col].map(fq_encode)\n",
    "    test[col+'_total']  = test[col].map(fq_encode)\n",
    "\n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "i_cols = ['uid']\n",
    "\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        new_column = col + '_' + period\n",
    "            \n",
    "        temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n",
    "        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n",
    "        fq_encode = temp_df[new_column].value_counts().to_dict()\n",
    "            \n",
    "        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n",
    "        test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n",
    "        \n",
    "        train[new_column] /= train[period+'_total']\n",
    "        test[new_column]  /= test[period+'_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionDT', 'TransactionAmt', 'card1_TransactionAmt_mean',\n",
       "       'card1_TransactionAmt_std', 'card2_TransactionAmt_mean',\n",
       "       'card2_TransactionAmt_std', 'card3_TransactionAmt_mean',\n",
       "       'card3_TransactionAmt_std', 'card5_TransactionAmt_mean',\n",
       "       'card5_TransactionAmt_std', 'uid_TransactionAmt_mean',\n",
       "       'uid_TransactionAmt_std', 'uid2_TransactionAmt_mean',\n",
       "       'uid2_TransactionAmt_std', 'uid3_TransactionAmt_mean',\n",
       "       'uid3_TransactionAmt_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transaction 관련 칼럼 확인\n",
    "train.columns[train.columns.str.contains('Transaction')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy columns 제거? 0916에선 안했었는데 이번엔 해보기\n",
    "noisy_cols = [#'TransactionID',\n",
    "    'TransactionDT', # Not target in features\n",
    "    'uid','uid2','uid3',                                 \n",
    "    'DT','DT_M','DT_W','DT_D',       \n",
    "    'DT_hour','DT_day_week','DT_day',\n",
    "    'DT_D_total','DT_W_total','DT_M_total',\n",
    "    'id_30','id_31','id_33',\n",
    "    'D1', 'D2', 'D9',\n",
    "]\n",
    "\n",
    "noisy_cat_cols = list(train[noisy_cols].select_dtypes(include=['object']).columns) \n",
    "noisy_num_cold = list(train[noisy_cols].select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting uid\n",
      "Deleting uid2\n",
      "Deleting uid3\n",
      "Deleting id_30\n",
      "Deleting id_31\n",
      "Deleting id_33\n",
      "Deleting TransactionDT\n",
      "Deleting DT\n",
      "Deleting DT_M\n",
      "Deleting DT_W\n",
      "Deleting DT_D\n",
      "Deleting DT_hour\n",
      "Deleting DT_day_week\n",
      "Deleting DT_day\n",
      "Deleting DT_D_total\n",
      "Deleting DT_W_total\n",
      "Deleting DT_M_total\n",
      "Deleting D1\n",
      "Deleting D2\n",
      "Deleting D9\n"
     ]
    }
   ],
   "source": [
    "cat_attr = list(train.select_dtypes(include=['object']).columns)\n",
    "num_attr = list(train.select_dtypes(exclude=['object']).columns)\n",
    "num_attr.remove('isFraud')\n",
    "\n",
    "for col in noisy_cat_cols:\n",
    "    if col in cat_attr:\n",
    "        print(\"Deleting \" + col)\n",
    "        cat_attr.remove(col)\n",
    "for col in noisy_num_cold:\n",
    "    if col in num_attr:\n",
    "        print(\"Deleting \" + col)\n",
    "        num_attr.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# object data -> Label Encoding(모델링 위해)\n",
    "# 그냥 팁: pandas에서 object보다 category 형 변수가 계산 빠름(변성윤님 블로그)?\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n",
    "        train[col] = le.transform(list(train[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 여기까지 FE 하고 데이터 저장\n",
    "train.to_pickle(\"../input/0925_train.pkl\")\n",
    "test.to_pickle(\"../input/0925_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 509)\n",
      "(506691, 508)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.87 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 여기서 TransactionDT 빼는 이유는 뭐지?? -> dtype 안 맞기 때문??\n",
    "# 일단 DT는 맞는 dtype이 아니라서 제거\n",
    "# 특이하게 여기서 시간이 몇 분 걸렸음.\n",
    "X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'DT'], axis=1)\n",
    "y = train.sort_values('TransactionDT')['isFraud']\n",
    "\n",
    "X_test = test.drop(['TransactionDT', 'DT'], axis=1)\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전에 학습시켰던 모델에서 상위 200개 feature만 걸러 보기\n",
    "X = X[feature_importances['feature'].head(200)]\n",
    "X_test = X_test[feature_importances['feature'].head(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 200)\n",
      "(506691, 200)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.006883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.96133\tvalid_1's auc: 0.892212\n",
      "[400]\ttraining's auc: 0.981253\tvalid_1's auc: 0.903979\n",
      "[600]\ttraining's auc: 0.991546\tvalid_1's auc: 0.913169\n",
      "[800]\ttraining's auc: 0.996319\tvalid_1's auc: 0.919103\n",
      "[1000]\ttraining's auc: 0.998494\tvalid_1's auc: 0.92226\n",
      "[1200]\ttraining's auc: 0.99942\tvalid_1's auc: 0.924321\n",
      "[1400]\ttraining's auc: 0.999776\tvalid_1's auc: 0.925346\n",
      "[1600]\ttraining's auc: 0.999916\tvalid_1's auc: 0.925968\n",
      "[1800]\ttraining's auc: 0.99997\tvalid_1's auc: 0.926229\n",
      "[2000]\ttraining's auc: 0.999991\tvalid_1's auc: 0.926344\n",
      "[2200]\ttraining's auc: 0.999997\tvalid_1's auc: 0.926522\n",
      "[2400]\ttraining's auc: 0.999999\tvalid_1's auc: 0.926891\n",
      "[2600]\ttraining's auc: 1\tvalid_1's auc: 0.926829\n",
      "[2800]\ttraining's auc: 1\tvalid_1's auc: 0.926925\n",
      "[3000]\ttraining's auc: 1\tvalid_1's auc: 0.92693\n",
      "[3200]\ttraining's auc: 1\tvalid_1's auc: 0.926805\n",
      "[3400]\ttraining's auc: 1\tvalid_1's auc: 0.926624\n",
      "Early stopping, best iteration is:\n",
      "[2971]\ttraining's auc: 1\tvalid_1's auc: 0.926962\n",
      "Fold 1 | AUC: 0.9269616867259733\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.96066\tvalid_1's auc: 0.917613\n",
      "[400]\ttraining's auc: 0.981072\tvalid_1's auc: 0.929444\n",
      "[600]\ttraining's auc: 0.992109\tvalid_1's auc: 0.936398\n",
      "[800]\ttraining's auc: 0.996869\tvalid_1's auc: 0.939928\n",
      "[1000]\ttraining's auc: 0.998848\tvalid_1's auc: 0.941566\n",
      "[1200]\ttraining's auc: 0.999599\tvalid_1's auc: 0.94253\n",
      "[1400]\ttraining's auc: 0.999859\tvalid_1's auc: 0.94331\n",
      "[1600]\ttraining's auc: 0.999953\tvalid_1's auc: 0.943675\n",
      "[1800]\ttraining's auc: 0.999986\tvalid_1's auc: 0.944089\n",
      "[2000]\ttraining's auc: 0.999996\tvalid_1's auc: 0.944119\n",
      "[2200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.944281\n",
      "[2400]\ttraining's auc: 1\tvalid_1's auc: 0.944185\n",
      "[2600]\ttraining's auc: 1\tvalid_1's auc: 0.944198\n",
      "Early stopping, best iteration is:\n",
      "[2204]\ttraining's auc: 0.999999\tvalid_1's auc: 0.9443\n",
      "Fold 2 | AUC: 0.9443004854141929\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.962027\tvalid_1's auc: 0.91643\n",
      "[400]\ttraining's auc: 0.981674\tvalid_1's auc: 0.927948\n",
      "[600]\ttraining's auc: 0.992305\tvalid_1's auc: 0.934218\n",
      "[800]\ttraining's auc: 0.996862\tvalid_1's auc: 0.936891\n",
      "[1000]\ttraining's auc: 0.998808\tvalid_1's auc: 0.938187\n",
      "[1200]\ttraining's auc: 0.999567\tvalid_1's auc: 0.938798\n",
      "[1400]\ttraining's auc: 0.999847\tvalid_1's auc: 0.939138\n",
      "[1600]\ttraining's auc: 0.999947\tvalid_1's auc: 0.939182\n",
      "[1800]\ttraining's auc: 0.999984\tvalid_1's auc: 0.939356\n",
      "[2000]\ttraining's auc: 0.999996\tvalid_1's auc: 0.939412\n",
      "[2200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.939357\n",
      "[2400]\ttraining's auc: 1\tvalid_1's auc: 0.939431\n",
      "Early stopping, best iteration is:\n",
      "[1946]\ttraining's auc: 0.999994\tvalid_1's auc: 0.939457\n",
      "Fold 3 | AUC: 0.9394565089139149\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.960049\tvalid_1's auc: 0.929183\n",
      "[400]\ttraining's auc: 0.980972\tvalid_1's auc: 0.942406\n",
      "[600]\ttraining's auc: 0.992045\tvalid_1's auc: 0.949404\n",
      "[800]\ttraining's auc: 0.99684\tvalid_1's auc: 0.952676\n",
      "[1000]\ttraining's auc: 0.998811\tvalid_1's auc: 0.954049\n",
      "[1200]\ttraining's auc: 0.999586\tvalid_1's auc: 0.954824\n",
      "[1400]\ttraining's auc: 0.999858\tvalid_1's auc: 0.955399\n",
      "[1600]\ttraining's auc: 0.999952\tvalid_1's auc: 0.955651\n",
      "[1800]\ttraining's auc: 0.999986\tvalid_1's auc: 0.955824\n",
      "[2000]\ttraining's auc: 0.999996\tvalid_1's auc: 0.955774\n",
      "[2200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.955747\n",
      "Early stopping, best iteration is:\n",
      "[1821]\ttraining's auc: 0.999988\tvalid_1's auc: 0.955881\n",
      "Fold 4 | AUC: 0.9558807328160434\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's auc: 0.962269\tvalid_1's auc: 0.90661\n",
      "[400]\ttraining's auc: 0.981438\tvalid_1's auc: 0.919713\n",
      "[600]\ttraining's auc: 0.992075\tvalid_1's auc: 0.927335\n",
      "[800]\ttraining's auc: 0.99679\tvalid_1's auc: 0.930711\n",
      "[1000]\ttraining's auc: 0.998794\tvalid_1's auc: 0.931974\n",
      "[1200]\ttraining's auc: 0.999579\tvalid_1's auc: 0.932929\n",
      "[1400]\ttraining's auc: 0.999852\tvalid_1's auc: 0.933402\n",
      "[1600]\ttraining's auc: 0.99995\tvalid_1's auc: 0.933807\n",
      "[1800]\ttraining's auc: 0.999984\tvalid_1's auc: 0.933993\n",
      "[2000]\ttraining's auc: 0.999996\tvalid_1's auc: 0.934228\n",
      "[2200]\ttraining's auc: 0.999999\tvalid_1's auc: 0.934518\n",
      "[2400]\ttraining's auc: 1\tvalid_1's auc: 0.934652\n",
      "[2600]\ttraining's auc: 1\tvalid_1's auc: 0.934909\n",
      "[2800]\ttraining's auc: 1\tvalid_1's auc: 0.935016\n",
      "[3000]\ttraining's auc: 1\tvalid_1's auc: 0.935139\n",
      "[3200]\ttraining's auc: 1\tvalid_1's auc: 0.935056\n",
      "[3400]\ttraining's auc: 1\tvalid_1's auc: 0.935149\n",
      "[3600]\ttraining's auc: 1\tvalid_1's auc: 0.935034\n",
      "[3800]\ttraining's auc: 1\tvalid_1's auc: 0.934973\n",
      "Early stopping, best iteration is:\n",
      "[3329]\ttraining's auc: 1\tvalid_1's auc: 0.935155\n",
      "Fold 5 | AUC: 0.935155020375476\n",
      "\n",
      "Mean AUC = 0.94035088684912\n",
      "Out of folds AUC = 0.9385723577984031\n",
      "Wall time: 2h 11min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NFOLDS = 5\n",
    "folds = KFold(n_splits=NFOLDS)\n",
    "\n",
    "columns = X.columns\n",
    "splits = folds.split(X, y)\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X.shape[0])\n",
    "score = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "  \n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n",
    "    \n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n",
    "    \n",
    "    y_pred_valid = clf.predict(X_valid)\n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n",
    "    \n",
    "    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n",
    "    y_preds += clf.predict(X_test) / NFOLDS\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"\\nMean AUC = {score}\")\n",
    "print(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score가 조금씩 낮은데 0.9476보다 높진 않을듯..\n",
    "folder_path = '../input/'\n",
    "sub = pd.read_csv(f'{folder_path}sample_submission.csv')\n",
    "sub['isFraud'] = y_preds\n",
    "sub.to_csv(\"0925_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
