{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ieee-cis Fraud Detection\n",
    "- 시각화를 제외한 데이터 전처리 / 모델링 부분 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['df_id.pkl', 'df_test.pkl', 'df_train.pkl', 'df_trans.pkl', 'sample_submission.csv', 'test_identity.csv', 'test_transaction.csv', 'train_identity.csv', 'train_transaction.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#standard plotly imports\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "#import cufflinks\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "#using plotly + cufflinks in offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "#preprocessing, modeling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "\n",
    "#hyperparameter optimization modules\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import gc\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "        end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542.35Mb\n",
      "25.86Mb\n",
      "Wall time: 2.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df_trans.to_pickle(\"../input/df_trans.pkl\")\n",
    "# df_id.to_pickle(\"../input/df_id.pkl\")\n",
    "df_trans = pd.read_pickle(\"../input/df_trans.pkl\")\n",
    "df_id = pd.read_pickle(\"../input/df_id.pkl\")\n",
    "print(\"{:1.2f}Mb\".format(df_trans.memory_usage().sum() / 1024**2)) # should be 542.35\n",
    "print(\"{:1.2f}Mb\".format(df_id.memory_usage().sum() / 1024**2)) # should be 25.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (590540, 394)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>dtypes</th>\n",
       "      <th>Missing</th>\n",
       "      <th>Uniques</th>\n",
       "      <th>First Value</th>\n",
       "      <th>Second Value</th>\n",
       "      <th>Third Value</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionID</td>\n",
       "      <td>int32</td>\n",
       "      <td>0</td>\n",
       "      <td>590540</td>\n",
       "      <td>2987000</td>\n",
       "      <td>2987001</td>\n",
       "      <td>2987002</td>\n",
       "      <td>19.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isFraud</td>\n",
       "      <td>int8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TransactionDT</td>\n",
       "      <td>int32</td>\n",
       "      <td>0</td>\n",
       "      <td>573349</td>\n",
       "      <td>86400</td>\n",
       "      <td>86401</td>\n",
       "      <td>86469</td>\n",
       "      <td>19.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TransactionAmt</td>\n",
       "      <td>float16</td>\n",
       "      <td>0</td>\n",
       "      <td>8195</td>\n",
       "      <td>68.5</td>\n",
       "      <td>29</td>\n",
       "      <td>59</td>\n",
       "      <td>8.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ProductCD</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>card1</td>\n",
       "      <td>int16</td>\n",
       "      <td>0</td>\n",
       "      <td>13553</td>\n",
       "      <td>13926</td>\n",
       "      <td>2755</td>\n",
       "      <td>4663</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>card2</td>\n",
       "      <td>float16</td>\n",
       "      <td>8933</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>404</td>\n",
       "      <td>490</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>card3</td>\n",
       "      <td>float16</td>\n",
       "      <td>1565</td>\n",
       "      <td>114</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>card4</td>\n",
       "      <td>object</td>\n",
       "      <td>1577</td>\n",
       "      <td>4</td>\n",
       "      <td>discover</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>visa</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>card5</td>\n",
       "      <td>float16</td>\n",
       "      <td>4259</td>\n",
       "      <td>119</td>\n",
       "      <td>142</td>\n",
       "      <td>102</td>\n",
       "      <td>166</td>\n",
       "      <td>2.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>card6</td>\n",
       "      <td>object</td>\n",
       "      <td>1571</td>\n",
       "      <td>4</td>\n",
       "      <td>credit</td>\n",
       "      <td>credit</td>\n",
       "      <td>debit</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>addr1</td>\n",
       "      <td>float16</td>\n",
       "      <td>65706</td>\n",
       "      <td>332</td>\n",
       "      <td>315</td>\n",
       "      <td>325</td>\n",
       "      <td>330</td>\n",
       "      <td>5.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>addr2</td>\n",
       "      <td>float16</td>\n",
       "      <td>65706</td>\n",
       "      <td>74</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dist1</td>\n",
       "      <td>float16</td>\n",
       "      <td>352271</td>\n",
       "      <td>2412</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>287</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dist2</td>\n",
       "      <td>float16</td>\n",
       "      <td>552913</td>\n",
       "      <td>1699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P_emaildomain</td>\n",
       "      <td>object</td>\n",
       "      <td>94456</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>R_emaildomain</td>\n",
       "      <td>object</td>\n",
       "      <td>453249</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C1</td>\n",
       "      <td>float16</td>\n",
       "      <td>0</td>\n",
       "      <td>1495</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C2</td>\n",
       "      <td>float16</td>\n",
       "      <td>0</td>\n",
       "      <td>1167</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C3</td>\n",
       "      <td>float16</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name   dtypes  Missing  Uniques First Value Second Value  \\\n",
       "0    TransactionID    int32        0   590540     2987000      2987001   \n",
       "1          isFraud     int8        0        2           0            0   \n",
       "2    TransactionDT    int32        0   573349       86400        86401   \n",
       "3   TransactionAmt  float16        0     8195        68.5           29   \n",
       "4        ProductCD   object        0        5           W            W   \n",
       "5            card1    int16        0    13553       13926         2755   \n",
       "6            card2  float16     8933      500         NaN          404   \n",
       "7            card3  float16     1565      114         150          150   \n",
       "8            card4   object     1577        4    discover   mastercard   \n",
       "9            card5  float16     4259      119         142          102   \n",
       "10           card6   object     1571        4      credit       credit   \n",
       "11           addr1  float16    65706      332         315          325   \n",
       "12           addr2  float16    65706       74          87           87   \n",
       "13           dist1  float16   352271     2412          19          NaN   \n",
       "14           dist2  float16   552913     1699         NaN          NaN   \n",
       "15   P_emaildomain   object    94456       59         NaN    gmail.com   \n",
       "16   R_emaildomain   object   453249       60         NaN          NaN   \n",
       "17              C1  float16        0     1495           1            1   \n",
       "18              C2  float16        0     1167           1            1   \n",
       "19              C3  float16        0       27           0            0   \n",
       "\n",
       "    Third Value  Entropy  \n",
       "0       2987002    19.17  \n",
       "1             0     0.22  \n",
       "2         86469    19.11  \n",
       "3            59     8.10  \n",
       "4             W     1.28  \n",
       "5          4663     9.97  \n",
       "6           490     6.32  \n",
       "7           150     0.68  \n",
       "8          visa     1.09  \n",
       "9           166     2.66  \n",
       "10        debit     0.82  \n",
       "11          330     5.06  \n",
       "12           87     0.08  \n",
       "13          287     6.33  \n",
       "14          NaN     7.41  \n",
       "15  outlook.com     2.68  \n",
       "16          NaN     2.76  \n",
       "17            1     2.72  \n",
       "18            1     2.75  \n",
       "19            0     0.04  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumetable(df_trans)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card3, card5\n",
    "df_trans.loc[df_trans['card3'].isin(df_trans['card3'].value_counts()[\n",
    "    df_trans['card3'].value_counts() < 200].index), 'card3'] = 'Others'\n",
    "df_trans.loc[df_trans['card5'].isin(df_trans['card5'].value_counts()[\n",
    "    df_trans['card5'].value_counts() < 300].index), 'card5'] = 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1 ~ M9 칼럼 null 채우기\n",
    "for col in ['M1','M2','M3','M4','M5','M6','M7','M8','M9']:\n",
    "    df_trans[col] = df_trans[col].fillna('Miss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addr1, addr2\n",
    "df_trans.loc[df_trans['addr1'].isin(\n",
    "    df_trans['addr1'].value_counts()[\n",
    "        df_trans['addr1'].value_counts() <= 5000].index), 'addr1'] = \"Others\"\n",
    "df_trans.loc[df_trans['addr2'].isin(\n",
    "    df_trans['addr2'].value_counts()[\n",
    "        df_trans['addr2'].value_counts() <= 50].index), 'addr2'] = \"Others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P_emaildomain 통합\n",
    "df_trans.loc[df_trans['P_emaildomain'].str[:5] == 'gmail',\n",
    "             'P_emaildomain'] = 'Google'\n",
    "df_trans.loc[df_trans['P_emaildomain'].str[:5] == 'yahoo',\n",
    "             'P_emaildomain'] = 'Yahoo Mail'\n",
    "df_trans.loc[df_trans['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n",
    "                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n",
    "                                         'outlook.es', 'live.com', 'live.fr',\n",
    "                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\n",
    "\n",
    "df_trans.loc[df_trans['P_emaildomain'].isin(\n",
    "    df_trans.P_emaildomain.value_counts()[df_trans.P_emaildomain.value_counts() <= 500].index),\n",
    "            'P_emaildomain'] = 'Others'\n",
    "df_trans.P_emaildomain.fillna(\"NoInf\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_emaildomain 통합\n",
    "df_trans.loc[df_trans['R_emaildomain'].str[:5] == 'gmail',\n",
    "             'R_emaildomain'] = 'Google'\n",
    "df_trans.loc[df_trans['R_emaildomain'].str[:5] == 'yahoo',\n",
    "             'R_emaildomain'] = 'Yahoo Mail'\n",
    "df_trans.loc[df_trans['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n",
    "                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n",
    "                                         'outlook.es', 'live.com', 'live.fr',\n",
    "                                         'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\n",
    "\n",
    "df_trans.loc[df_trans['R_emaildomain'].isin(\n",
    "    df_trans.R_emaildomain.value_counts()[df_trans.R_emaildomain.value_counts() <= 300].index),\n",
    "            'R_emaildomain'] = 'Others'\n",
    "df_trans.R_emaildomain.fillna(\"NoInf\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1, C2 통합\n",
    "df_trans.loc[df_trans['C1'].isin(df_trans['C1'].value_counts()[\n",
    "    df_trans['C1'].value_counts() <= 400].index), 'C1'] = \"Others\"\n",
    "df_trans.loc[df_trans['C2'].isin(df_trans['C2'].value_counts()[\n",
    "    df_trans['C2'].value_counts() <= 350].index), 'C2'] = \"Others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2017-12-02 00:00:00\n",
      "1   2017-12-02 00:00:01\n",
      "2   2017-12-02 00:01:09\n",
      "3   2017-12-02 00:01:39\n",
      "4   2017-12-02 00:01:46\n",
      "Name: Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# TransactionDT 처리\n",
    "import datetime\n",
    "\n",
    "START_DATE = \"2017-12-01\"\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "df_trans['Date'] = df_trans['TransactionDT'].apply(\n",
    "lambda x: (startdate + datetime.timedelta(seconds = x))) #DT 값은 초??\n",
    "\n",
    "print(df_trans['Date'].head()) # 이렇게 시간까지 표시됨\n",
    "\n",
    "df_trans['_Weekdays'] = df_trans['Date'].dt.dayofweek #요일\n",
    "df_trans['_Hours'] = df_trans['Date'].dt.hour #시간\n",
    "df_trans['_Days'] = df_trans['Date'].dt.day #날(그 달의 몇번째 날인지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_12</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_14</th>\n",
       "      <th>id_15</th>\n",
       "      <th>id_16</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_18</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>id_21</th>\n",
       "      <th>...</th>\n",
       "      <th>id_29</th>\n",
       "      <th>id_30</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>144233</td>\n",
       "      <td>127320.0</td>\n",
       "      <td>80044.0</td>\n",
       "      <td>140985</td>\n",
       "      <td>129340</td>\n",
       "      <td>139369.0</td>\n",
       "      <td>4.511300e+04</td>\n",
       "      <td>139318.0</td>\n",
       "      <td>139261.0</td>\n",
       "      <td>5159.0</td>\n",
       "      <td>...</td>\n",
       "      <td>140978</td>\n",
       "      <td>77565</td>\n",
       "      <td>140282</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>73289</td>\n",
       "      <td>77805</td>\n",
       "      <td>140985</td>\n",
       "      <td>140985</td>\n",
       "      <td>140985</td>\n",
       "      <td>140985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NotFound</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Found</td>\n",
       "      <td>Found</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Found</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>chrome 63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1920x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>123025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67728</td>\n",
       "      <td>66324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>74926</td>\n",
       "      <td>21155</td>\n",
       "      <td>22000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16874</td>\n",
       "      <td>60011</td>\n",
       "      <td>77814</td>\n",
       "      <td>134066</td>\n",
       "      <td>110452</td>\n",
       "      <td>73922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.561523e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-660.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>49.0</td>\n",
       "      <td>-360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>266.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>341.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>427.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>486.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.0</td>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>671.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>854.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_12     id_13    id_14   id_15   id_16     id_17         id_18  \\\n",
       "count     144233  127320.0  80044.0  140985  129340  139369.0  4.511300e+04   \n",
       "unique         2       NaN      NaN       3       2       NaN           NaN   \n",
       "top     NotFound       NaN      NaN   Found   Found       NaN           NaN   \n",
       "freq      123025       NaN      NaN   67728   66324       NaN           NaN   \n",
       "mean         NaN       NaN      NaN     NaN     NaN       NaN           inf   \n",
       "std          NaN       0.0      NaN     NaN     NaN       0.0  1.561523e+00   \n",
       "min          NaN      10.0   -660.0     NaN     NaN     100.0  1.000000e+01   \n",
       "25%          NaN      49.0   -360.0     NaN     NaN     166.0  1.300000e+01   \n",
       "50%          NaN      52.0   -300.0     NaN     NaN     166.0  1.500000e+01   \n",
       "75%          NaN      52.0   -300.0     NaN     NaN     225.0  1.500000e+01   \n",
       "max          NaN      64.0    720.0     NaN     NaN     229.0  2.900000e+01   \n",
       "\n",
       "           id_19     id_20   id_21  ...   id_29       id_30        id_31  \\\n",
       "count   139318.0  139261.0  5159.0  ...  140978       77565       140282   \n",
       "unique       NaN       NaN     NaN  ...       2          75          130   \n",
       "top          NaN       NaN     NaN  ...   Found  Windows 10  chrome 63.0   \n",
       "freq         NaN       NaN     NaN  ...   74926       21155        22000   \n",
       "mean         NaN       NaN     inf  ...     NaN         NaN          NaN   \n",
       "std          NaN       NaN     inf  ...     NaN         NaN          NaN   \n",
       "min        100.0     100.0   100.0  ...     NaN         NaN          NaN   \n",
       "25%        266.0     256.0   252.0  ...     NaN         NaN          NaN   \n",
       "50%        341.0     472.0   252.0  ...     NaN         NaN          NaN   \n",
       "75%        427.0     533.0   486.5  ...     NaN         NaN          NaN   \n",
       "max        671.0     661.0   854.0  ...     NaN         NaN          NaN   \n",
       "\n",
       "          id_32      id_33           id_34   id_35   id_36   id_37   id_38  \n",
       "count   77586.0      73289           77805  140985  140985  140985  140985  \n",
       "unique      NaN        260               4       2       2       2       2  \n",
       "top         NaN  1920x1080  match_status:2       T       F       T       F  \n",
       "freq        NaN      16874           60011   77814  134066  110452   73922  \n",
       "mean        NaN        NaN             NaN     NaN     NaN     NaN     NaN  \n",
       "std         0.0        NaN             NaN     NaN     NaN     NaN     NaN  \n",
       "min         0.0        NaN             NaN     NaN     NaN     NaN     NaN  \n",
       "25%        24.0        NaN             NaN     NaN     NaN     NaN     NaN  \n",
       "50%        24.0        NaN             NaN     NaN     NaN     NaN     NaN  \n",
       "75%        32.0        NaN             NaN     NaN     NaN     NaN     NaN  \n",
       "max        32.0        NaN             NaN     NaN     NaN     NaN     NaN  \n",
       "\n",
       "[11 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe -> 기본값은 null(NaN) 제외함 -> include='all'\n",
    "df_id[['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n",
    "       'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n",
    "       'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n",
    "       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 434)\n",
      "(506691, 433)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "df_trans = pd.read_csv('../input/train_transaction.csv')\n",
    "df_test_trans = pd.read_csv(\"../input/test_transaction.csv\")\n",
    "\n",
    "# test\n",
    "df_id = pd.read_csv('../input/train_identity.csv')\n",
    "df_test_id = pd.read_csv(\"../input/test_identity.csv\")\n",
    "\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\",\n",
    "                               index_col = 'TransactionID')\n",
    "\n",
    "df_train = df_trans.merge(df_id, how='left', left_index=True,\n",
    "                         right_index=True, on='TransactionID')\n",
    "df_test = df_test_trans.merge(df_test_id, how='left', left_index=True,\n",
    "                         right_index=True, on='TransactionID')\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape) # train,test 행 수가 별 차이가 없음\n",
    "\n",
    "del df_trans, df_id, df_test_trans, df_test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 645.97 Mb (67.0% reduction)\n",
      "Mem. usage decreased to 561.50 Mb (66.5% reduction)\n",
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645.97 Mb\n",
      "561.50 Mb\n",
      "Wall time: 8.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df_train.to_pickle(\"../input/df_train.pkl\")\n",
    "# df_test.to_pickle(\"../input/df_test.pkl\")\n",
    "df_train = pd.read_pickle(\"../input/df_train.pkl\")\n",
    "df_test = pd.read_pickle(\"../input/df_test.pkl\")\n",
    "#must be 645.97\n",
    "print(\"{:1.2f} Mb\".format(df_train.memory_usage().sum() / 1024**2))\n",
    "#must be 561.50\n",
    "print(\"{:1.2f} Mb\".format(df_test.memory_usage().sum() / 1024**2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## email domain mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.P_emaildomain.fillna(\"NoInf\", inplace=True)\n",
    "# df_test.R_emaildomain.fillna(\"NoInf\", inplace=True)\n",
    "\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    # bin -> emails dict에 따라 매핑(수정)\n",
    "    df_train[c + '_bin'] = df_train[c].map(emails)\n",
    "    df_test[c + '_bin'] = df_test[c].map(emails)\n",
    "    \n",
    "    # suffix -> 도메인 중 맨 마지막(. 뒤에) 부분\n",
    "    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    # suffix 확인 -> us_email 값에 해당 안하면 그대로. 해당하면 'us'로\n",
    "    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>P_emaildomain_bin</th>\n",
       "      <th>P_emaildomain_suffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmail.com</td>\n",
       "      <td>google</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outlook.com</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gmail.com</td>\n",
       "      <td>google</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  P_emaildomain P_emaildomain_bin P_emaildomain_suffix\n",
       "0           NaN               NaN                  nan\n",
       "1     gmail.com            google                  com\n",
       "2   outlook.com         microsoft                  com\n",
       "3     yahoo.com             yahoo                  com\n",
       "4     gmail.com            google                  com"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "df_train[['P_emaildomain', 'P_emaildomain_bin', 'P_emaildomain_suffix']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com    466477\n",
       "nan     94456\n",
       "us      25038\n",
       "mx       2499\n",
       "es        877\n",
       "de        506\n",
       "fr        494\n",
       "uk        161\n",
       "jp         32\n",
       "Name: P_emaildomain_suffix, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.P_emaildomain_suffix.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding. 근데 숫자로 된 범주형 변수들은 안하나?\n",
    "for f in df_train.drop('isFraud', axis=1).columns:\n",
    "    if df_train[f].dtype == 'object' or df_test[f].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df_train[f].values) + list(df_test[f].values)) #더하면 어떻게 되는거지?\n",
    "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "        df_test[f] = lbl.transform(list(df_test[f].values))\n",
    "        # 이렇게 되면 NaN은 NaN으로 transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['card6'].isnull().sum()\n",
    "# object 형식인 card6의 null이 없어짐(원래는 1571개). Label Encoding하는 목적인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화\n",
    "df_train['Trans_min_std'] = (df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()) / df_train['TransactionAmt'].std()\n",
    "df_test['Trans_min_std'] = (df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()) / df_test['TransactionAmt'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card1 별 TransactionAmt 그룹평균으로 나눈 값\n",
    "df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt']\\\n",
    "         / df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "# card1 별 TransactionAmt std로 나눈 값. 이것들 왜 하는건지??\n",
    "df_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt']\\\n",
    "         / df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "df_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt']\\\n",
    "         / df_train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "df_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt']\\\n",
    "         / df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "         \n",
    "df_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt']\\\n",
    "         / df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "df_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt']\\\n",
    "         / df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "df_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt']\\\n",
    "         / df_test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "df_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt']\\\n",
    "         / df_test.groupby(['card4'])['TransactionAmt'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범위 맞춰주기 위해 TransactionAmt는 log\n",
    "df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\n",
    "df_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concating dfs to get PCA of V features\n",
    "- V1 ~ V339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['isFraud'] = 'test' # test용으로 열 추가\n",
    "df = pd.concat([df_train, df_test], axis=0, sort=False) #단순히 행 합침\n",
    "df = df.reset_index()\n",
    "df = df.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_change(df, cols, n_components, prefix=\"PCA_\", rand_seed=4):\n",
    "    pca = PCA(n_components=n_components, random_state=rand_seed)\n",
    "    principalComponents = pca.fit_transform(df[cols])\n",
    "    print(np.cumsum(pca.explained_variance_ratio_)) #분산 보존 확인\n",
    "    principalDf = pd.DataFrame(principalComponents)\n",
    "    \n",
    "    df.drop(cols, axis=1, inplace=True) #pca 안된 원래 칼럼은 drop\n",
    "    principalDf.rename(columns = lambda x: str(prefix) + str(x),\n",
    "                      inplace=True)\n",
    "    df = pd.concat([df, principalDf], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_v = df_train.columns[55:394] # V1 ~ V339 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49523164 0.72790493 0.82472656 0.87137364 0.89341487 0.91022079\n",
      " 0.92581611 0.93692118 0.94504422 0.95212088 0.95860489 0.96192865\n",
      " 0.96480405 0.96708473 0.96886368 0.97048424 0.97205917 0.97349095\n",
      " 0.9748767  0.97607318 0.97715478 0.97821595 0.97917455 0.98004338\n",
      " 0.98084914 0.98162228 0.98233325 0.98300678 0.98362971 0.98424198]\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "for col in mas_v:\n",
    "    df[col] = df[col].fillna(df[col].min() - 2) # -2 하는 이유는?\n",
    "    df[col] = minmax_scale(df[col], feature_range=(0, 1))\n",
    "    \n",
    "df = PCA_change(df, mas_v, prefix=\"PCA_V_\", n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.to_pickle(\"../input/df_pca.pkl\")\n",
    "df = pd.read_pickle(\"../input/df_pca.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 272.06 Mb (61.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "# 사실 30개까지 안해도 될듯\n",
    "df = reduce_mem_usage(df)\n",
    "# nan value들 때문에 오류 발생. 커널은 어떻게 괜찮은지??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting train and test back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop(\n",
    "'isFraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 134)\n",
      "(506691, 133)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape) # 정규화 과정에서 한 과정 합쳤기 때문에 열이 하나 적음\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.sort_values('TransactionDT').drop([\n",
    "    'isFraud', 'TransactionDT'\n",
    "], axis=1)\n",
    "\n",
    "y_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n",
    "\n",
    "X_test = df_test.sort_values('TransactionDT').drop(['TransactionDT'],\n",
    "                                                  axis=1)\n",
    "\n",
    "del df_train\n",
    "df_test = df_test[['TransactionDT']] #이건 왜? label 위한 용도?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperOpt 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from xgboost import plot_importance\n",
    "import time\n",
    "\n",
    "def objective(params):\n",
    "    time1 = time.time()\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'subsample': \"{:.2f}\".format(params['subsample']),\n",
    "        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
    "        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
    "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
    "        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
    "        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
    "    }\n",
    "\n",
    "    print(\"\\n############## New Run ################\")\n",
    "    print(f\"params = {params}\")\n",
    "    FOLDS = 7\n",
    "    count=1\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=FOLDS)\n",
    "    y_preds = np.zeros(sample_submission.shape[0])\n",
    "    y_oof = np.zeros(X_train.shape[0])\n",
    "    score_mean = 0\n",
    "    for tr_idx, val_idx in tss.split(X_train, y_train):\n",
    "        clf = xgb.XGBClassifier(\n",
    "            n_estimators=600, random_state=4, verbose=True, \n",
    "            tree_method='gpu_hist', \n",
    "            **params\n",
    "        )\n",
    "\n",
    "        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
    "        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
    "        #print(y_pred_train)\n",
    "        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n",
    "        # plt.show()\n",
    "        score_mean += score\n",
    "        print(f'{count} CV - score: {round(score, 4)}')\n",
    "        count += 1\n",
    "    time2 = time.time() - time1\n",
    "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
    "    gc.collect()\n",
    "    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n",
    "    del X_tr, X_vl, y_tr, y_vl, clf, score\n",
    "    return -(score_mean / FOLDS)\n",
    "\n",
    "\n",
    "space = {\n",
    "    # The maximum depth of a tree, same as GBM.\n",
    "    # Used to control over-fitting as higher depth will allow model \n",
    "    # to learn relations very specific to a particular sample.\n",
    "    # Should be tuned using CV.\n",
    "    # Typical values: 3-10\n",
    "    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n",
    "    \n",
    "    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n",
    "    # (meaning pulling weights to 0). It can be more useful when the objective\n",
    "    # is logistic regression since you might need help with feature selection.\n",
    "    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
    "    \n",
    "    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n",
    "    # approach can be more useful in tree-models where zeroing \n",
    "    # features might not make much sense.\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
    "    \n",
    "    # eta: Analogous to learning rate in GBM\n",
    "    # Makes the model more robust by shrinking the weights on each step\n",
    "    # Typical final values to be used: 0.01-0.2\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    \n",
    "    # colsample_bytree: Similar to max_features in GBM. Denotes the \n",
    "    # fraction of columns to be randomly samples for each tree.\n",
    "    # Typical values: 0.5-1\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n",
    "    \n",
    "    # A node is split only when the resulting split gives a positive\n",
    "    # reduction in the loss function. Gamma specifies the \n",
    "    # minimum loss reduction required to make a split.\n",
    "    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "    'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "    \n",
    "    # more increases accuracy, but may lead to overfitting.\n",
    "    # num_leaves: the number of leaf nodes to use. Having a large number \n",
    "    # of leaves will improve accuracy, but will also lead to overfitting.\n",
    "    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
    "    \n",
    "    # specifies the minimum samples per leaf node.\n",
    "    # the minimum number of samples (data) to group into a leaf. \n",
    "    # The parameter can greatly assist with overfitting: larger sample\n",
    "    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n",
    "    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n",
    "    \n",
    "    # subsample: represents a fraction of the rows (observations) to be \n",
    "    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n",
    "    # in their paper A Scalable Tree Boosting System recommend \n",
    "    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
    "    \n",
    "    # randomly select a fraction of the features.\n",
    "    # feature_fraction: controls the subsampling of features used\n",
    "    # for training (as opposed to subsampling the actual training data in \n",
    "    # the case of bagging). Smaller fractions reduce overfitting.\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
    "    \n",
    "    # randomly bag or subsample training data.\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n",
    "    \n",
    "    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n",
    "    # of the training data. Both values need to be set for bagging to be used.\n",
    "    # The frequency controls how often (iteration) bagging is used. Smaller\n",
    "    # fractions and frequencies reduce overfitting.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\",\n",
    "                               index_col = 'TransactionID')\n",
    "\n",
    "# xgboost -> GPU 있어야 작동하는 듯\n",
    "# set algorithm parameters\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest,\n",
    "           max_evals=27)\n",
    "\n",
    "# print best parameters\n",
    "best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\",\n",
    "                               index_col = 'TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_cols = [\n",
    "    'TransactionID', 'TransactionDT', TARGET\n",
    "]\n",
    "\n",
    "features_columns = list(df_train)\n",
    "for col in rm_cols:\n",
    "    if col in features_columns:\n",
    "        features_columns.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':1,\n",
    "                    'n_estimators':800,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import os, sys, gc, warnings, random\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fitting, prediction 같이 하는 함수 -> train, test set 모두 필요\n",
    "def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "\n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold:',fold_)\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n",
    "            \n",
    "        print(len(tr_x),len(vl_x))\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            vl_data = lgb.Dataset(P, label=P_y) \n",
    "        else:\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)  \n",
    "\n",
    "        estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "        pp_p = estimator.predict(P)\n",
    "        predictions += pp_p/NFOLDS\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "            print(feature_imp)\n",
    "        \n",
    "        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "        gc.collect()\n",
    "        \n",
    "    tt_df['prediction']  = predictions\n",
    "    \n",
    "    return tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "TARGET = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: list(X_train) 하면 칼럼명들이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.merge(sample_submission, how='left', left_index=True,\n",
    "                         right_index=True, on='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "295270 295270\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.962621\tvalid_1's auc: 0.927216\n",
      "[400]\ttraining's auc: 0.988211\tvalid_1's auc: 0.944166\n",
      "[600]\ttraining's auc: 0.995675\tvalid_1's auc: 0.950878\n",
      "[800]\ttraining's auc: 0.998033\tvalid_1's auc: 0.953716\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's auc: 0.998033\tvalid_1's auc: 0.953716\n",
      "Fold: 1\n",
      "295270 295270\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.964362\tvalid_1's auc: 0.92579\n",
      "[400]\ttraining's auc: 0.989465\tvalid_1's auc: 0.941859\n",
      "[600]\ttraining's auc: 0.996079\tvalid_1's auc: 0.948411\n",
      "[800]\ttraining's auc: 0.998164\tvalid_1's auc: 0.951398\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's auc: 0.998164\tvalid_1's auc: 0.951398\n"
     ]
    }
   ],
   "source": [
    "if LOCAL_TEST:\n",
    "    test_predictions = make_predictions(df_train, df_test,\n",
    "                        features_columns, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "else:\n",
    "    lgb_params['learning_rate'] = 0.01\n",
    "    lgb_params['n_estimators'] = 800\n",
    "    lgb_params['early_stopping_rounds'] = 100    \n",
    "    test_predictions = make_predictions(df_train, df_test, features_columns, TARGET, lgb_params, NFOLDS=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOCAL_TEST 는 어디서 True로 변하는 건지?\n",
    "\n",
    "########################### Export\n",
    "if not LOCAL_TEST:\n",
    "    test_predictions['isFraud'] = test_predictions['prediction']\n",
    "    test_predictions[['TransactionID','isFraud']].to_csv('first_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590540    0.001922\n",
      "590541    0.002357\n",
      "590542    0.005210\n",
      "590543    0.004001\n",
      "590544    0.003321\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions['isFraud'].head()) #이렇게 확률 값이 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
